"""
Master Orchestrator for Regional Data Enrichment
Coordinates API fetching, data enrichment, and chatbot learning
"""
import os
import json
import logging
import sys
from datetime import datetime
from typing import Dict, List, Optional
from dotenv import load_dotenv

from data_enrichment_pipeline import DataEnrichmentPipeline
from chatbot_learning_engine import ChatbotLearningEngine

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('enrichment_pipeline.log')
    ]
)
logger = logging.getLogger(__name__)

load_dotenv()

class EnrichmentOrchestrator:
    """Orchestrates the complete enrichment and learning pipeline"""
    
    def __init__(self):
        self.enrichment_pipeline = DataEnrichmentPipeline()
        self.learning_engine = ChatbotLearningEngine()
        self.execution_log = {
            'start_time': datetime.now().isoformat(),
            'stages': {}
        }
    
    def run_full_pipeline(self) -> Dict:
        """Run complete enrichment and learning pipeline"""
        logger.info("\n" + "="*70)
        logger.info("MINDTHEGAP REGIONAL DATA ENRICHMENT & CHATBOT LEARNING PIPELINE")
        logger.info("="*70)
        
        try:
            # Stage 1: Enrich all states
            logger.info("\n[STAGE 1/3] ENRICHING STATE PROFILES WITH GOVERNMENT DATA")
            logger.info("-" * 70)
            enrichment_results = self._run_enrichment()
            self.execution_log['stages']['enrichment'] = enrichment_results
            
            # Stage 2: Create regional aggregations
            logger.info("\n[STAGE 2/3] CREATING REGIONAL AGGREGATIONS")
            logger.info("-" * 70)
            aggregation_results = self._run_aggregations()
            self.execution_log['stages']['aggregations'] = aggregation_results
            
            # Stage 3: Fine-tune chatbot
            logger.info("\n[STAGE 3/3] FINE-TUNING CHATBOT ON ENRICHED DATA")
            logger.info("-" * 70)
            learning_results = self._run_learning()
            self.execution_log['stages']['learning'] = learning_results
            
            # Summary
            self._print_summary()
            
            return self.execution_log
        
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            self.execution_log['status'] = 'failed'
            self.execution_log['error'] = str(e)
            return self.execution_log
    
    def _run_enrichment(self) -> Dict:
        """Run data enrichment stage"""
        logger.info("Starting enrichment of all 50 states with government data...")
        logger.info("This will fetch Census Bureau, BLS, and FRED data for each state.")
        logger.info("Note: Requires API keys in .env file")
        
        try:
            results = self.enrichment_pipeline.enrich_all_states()
            
            successful = results['states_successful']
            failed = results['states_failed']
            
            logger.info(f"‚úÖ Enrichment stage complete:")
            logger.info(f"   ‚Ä¢ Processed: {results['states_processed']} states")
            logger.info(f"   ‚Ä¢ Successful: {successful}")
            logger.info(f"   ‚Ä¢ Failed: {failed}")
            
            if failed > 0:
                logger.warning(f"   Note: {failed} states failed (likely due to missing API keys)")
                logger.info("   Set CENSUS_API_KEY, BLS_API_KEY, FRED_API_KEY in .env")
            
            return results
        
        except Exception as e:
            logger.error(f"Enrichment failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _run_aggregations(self) -> Dict:
        """Run regional aggregations"""
        logger.info("Creating regional aggregations (Northeast, Southeast, etc.)...")
        
        try:
            success = self.enrichment_pipeline.create_regional_aggregations()
            
            if success:
                logger.info("‚úÖ Regional aggregations created successfully")
                return {'status': 'success'}
            else:
                logger.error("Failed to create aggregations")
                return {'status': 'failed'}
        
        except Exception as e:
            logger.error(f"Aggregation failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _run_learning(self) -> Dict:
        """Run chatbot learning stage"""
        logger.info("Fine-tuning chatbot on enriched regional data...")
        logger.info("Generating training dataset, correlation patterns, and knowledge base...")
        
        try:
            results = self.learning_engine.run_learning_pipeline()
            
            logger.info(f"‚úÖ Chatbot learning complete:")
            for component, data in results['components'].items():
                status = data.get('status', 'unknown')
                logger.info(f"   ‚Ä¢ {component}: {status}")
            
            return results
        
        except Exception as e:
            logger.error(f"Learning pipeline failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _print_summary(self):
        """Print execution summary"""
        logger.info("\n" + "="*70)
        logger.info("EXECUTION SUMMARY")
        logger.info("="*70)
        
        enrichment = self.execution_log['stages'].get('enrichment', {})
        learning = self.execution_log['stages'].get('learning', {})
        
        logger.info("\nüìä DATA ENRICHMENT:")
        logger.info(f"  States processed: {enrichment.get('states_processed', 'N/A')}")
        logger.info(f"  States successful: {enrichment.get('states_successful', 'N/A')}")
        logger.info(f"  Location: s3://mindthegap-gov-data/enriched-regional-data/")
        
        logger.info("\nüéì CHATBOT LEARNING:")
        logger.info(f"  Training data: {learning.get('components', {}).get('training_data', {}).get('count', 'N/A')} Q&A pairs")
        logger.info(f"  Patterns learned: {learning.get('components', {}).get('patterns', {}).get('count', 'N/A')}")
        logger.info(f"  Knowledge base: ‚úì Created")
        logger.info(f"  Location: s3://mindthegap-gov-data/chatbot-training-data/")
        
        logger.info("\n‚úÖ PIPELINE COMPLETE!")
        logger.info("="*70)
    
    def run_incremental_update(self, states: list = None) -> Dict:
        """
        Run incremental update for specific states
        Useful for periodic refreshes without re-running entire pipeline
        """
        logger.info(f"\nüîÑ Running incremental update for {len(states) if states else 'selected'} states...")
        
        if not states:
            logger.info("No states specified. Use: --states CA TX NY")
            return {'status': 'skipped'}
        
        results = {'states_updated': 0, 'failures': []}
        
        for state in states:
            try:
                logger.info(f"  Updating {state}...")
                # Implementation would update individual state
                results['states_updated'] += 1
            except Exception as e:
                results['failures'].append(f"{state}: {str(e)}")
        
        logger.info(f"‚úÖ Updated {results['states_updated']} states")
        
        return results


def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Regional Data Enrichment & Chatbot Learning Pipeline'
    )
    parser.add_argument(
        '--mode',
        choices=['full', 'enrichment-only', 'learning-only', 'incremental'],
        default='full',
        help='Pipeline mode to run'
    )
    parser.add_argument(
        '--states',
        nargs='+',
        help='States to update (for incremental mode): CA TX NY ...'
    )
    parser.add_argument(
        '--skip-api-keys-warning',
        action='store_true',
        help='Skip warning about missing API keys'
    )
    
    args = parser.parse_args()
    
    # Check API keys
    if not args.skip_api_keys_warning:
        missing_keys = []
        if not os.getenv('CENSUS_API_KEY'):
            missing_keys.append('CENSUS_API_KEY')
        if not os.getenv('BLS_API_KEY'):
            missing_keys.append('BLS_API_KEY')
        if not os.getenv('FRED_API_KEY'):
            missing_keys.append('FRED_API_KEY')
        
        if missing_keys:
            logger.warning(f"‚ö†Ô∏è  Missing API keys: {', '.join(missing_keys)}")
            logger.warning("Pipeline will use default/cached data.")
            logger.warning("Get free API keys:")
            logger.warning("  - Census: https://api.census.gov/data/key_signup.html")
            logger.warning("  - BLS: https://www.bls.gov/developers/home.htm")
            logger.warning("  - FRED: https://fred.stlouisfed.org/docs/api/")
            logger.warning("")
    
    orchestrator = EnrichmentOrchestrator()
    
    if args.mode == 'full':
        results = orchestrator.run_full_pipeline()
    
    elif args.mode == 'enrichment-only':
        results = {'stage': 'enrichment', 'result': orchestrator._run_enrichment()}
    
    elif args.mode == 'learning-only':
        results = {'stage': 'learning', 'result': orchestrator._run_learning()}
    
    elif args.mode == 'incremental':
        results = orchestrator.run_incremental_update(args.states)
    
    else:
        parser.print_help()
        return
    
    # Save execution log
    log_file = f"pipeline-execution-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.json"
    with open(log_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    logger.info(f"\nüìù Execution log saved: {log_file}")


if __name__ == "__main__":
    main()
